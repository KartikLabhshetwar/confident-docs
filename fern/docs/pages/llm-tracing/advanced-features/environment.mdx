---
subtitle: Set your environments during tracing for better debugging
slug: llm-tracing/advanced-features/environment
---

## Overview

The environment feature allows you to specify which environment your traces are coming from. This is useful for separating traces from different environments in `"development"`, `"staging"`, `"production"`, or `"testing"`.

<Tip>
Traces from [component-level evals](/docs/llm-evaluation/single-turn/component-level) are automatically classified in the `"testing"` enviornment.
</Tip>

## Configure Environment

You can configure the environment with the `CONFIDENT_TRACE_ENVIRONMENT` environment variable.

```bash 
export CONFIDENT_TRACE_ENVIRONMENT="staging" 
```

Alternatively, you can set the environment directly in code:

<Tabs>
    <Tab title="Python" language="python">
            ```python title="main.py" {4}
            from openai import OpenAI 
            from deepeval.tracing import observe, trace_manager
            
            trace_manager.configure(environment="production")
            client = OpenAI()
            
            @observe()
            def llm_app(query: str):
                return client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
            llm_app("Write me a poem.")
            ```
    </Tab>
    <Tab title="TypeScript" language="typescript">
            ```typescript title="index.ts" {4}
            import OpenAI from 'openai';
            import { observe, traceManager } from 'deepeval-ts/tracing';

            traceManager.configure({ environment: "production" });
            const openai = new OpenAI();

            const llmApp = async (query: string) => {
                const result = await openai.chat.completions.create({ 
                    model: "gpt-4o", 
                    messages: [{ role: "user", content: query }] 
                })
                return result.choices[0].message.content;
            };

            const observedLlmApp = observe({ type: "llm", model: "gpt-4o", fn: llmApp });
            observedLlmApp("Write me a poem.");
            ```
    </Tab>

</Tabs>

The `environment` can be either `"production"`, `"staging"`, or `"development"`, and helps you identify where your traces are coming from.

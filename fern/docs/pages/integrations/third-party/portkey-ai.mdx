---
slug: integrations/third-party/portkey
subtitle: Portkey AI serves as a unified interface for interacting LLMs
---

## Overview

Confident AI lets you trace and evaluate Portkey LLM calls, whether standalone or used as a component within a larger application.

## Tracing Quickstart
<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

```bash
pip install -U deepeval portkey-ai
```

</Step>

<Step title="Setup Confident AI Key">

Login to Confident AI using your Confident API key.

<CodeBlocks>

<CodeBlock language="bash">

```bash
deepeval login
```

</CodeBlock>

<CodeBlock language="python">

```python
import deepeval

deepeval.login("<your-confident-api-key>")
```
</CodeBlock>

<CodeBlock title="Env" language="bash">
```bash
export CONFIDENT_API_KEY="<your-confident-api-key>"
```
</CodeBlock>

</CodeBlocks>

</Step>

<Step title="Configure Portkey">

To begin tracing your Portkey LLM calls as a component in your application, import OpenAI and use the `PORTKEY_GATEWAY_URL` to trace the calls.

<Tabs>
    <Tab title="Chat Completions">
        ```python main.py
        from deepeval.openai import OpenAI
        from portkey_ai import PORTKEY_GATEWAY_URL

        portkey = OpenAI(
        base_url = PORTKEY_GATEWAY_URL,
        api_key = "<PORTKEY_API_KEY>"
        )

        response = portkey.chat.completions.create(
            model = "@slug/<model>",
            messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is Portkey"}
            ],
        )
        ```
    </Tab>
    <Tab title="Async Chat Completions">
        ```python main.py
        import asyncio

        from deepeval.openai import AsyncOpenAI
        from portkey_ai import PORTKEY_GATEWAY_URL

        portkey = AsyncOpenAI(
            base_url = PORTKEY_GATEWAY_URL,
            api_key = "<PORTKEY_API_KEY>"
        )

        async def main():
        response = await portkey.chat.completions.create(
            model = "@slug/<model>",
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "What is Portkey"}
            ],
        )

        asyncio.run(main())        
        ```
    </Tab>
    <Tab title="Responses">
        ```python main.py
        from deepeval.openai import OpenAI
        from portkey_ai import PORTKEY_GATEWAY_URL

        portkey = OpenAI(
            base_url = PORTKEY_GATEWAY_URL,
            api_key = "<PORTKEY_API_KEY>"
        )

        response = portkey.responses.create(
            model = "@slug/<model>",
            instructions = "You are a helpful assistant.",
            input = "What is Portkey"
        )
        ```
    </Tab>
    <Tab title="Async Responses">
        ```python main.py
        from deepeval.openai import AsyncOpenAI
        from portkey_ai import PORTKEY_GATEWAY_URL

        portkey = AsyncOpenAI(
            base_url = PORTKEY_GATEWAY_URL,
            api_key = "<PORTKEY_API_KEY>"
        )

        async def main():
            response = await portkey.responses.create(
                model = "@slug/<model>",
                instructions = "You are a helpful assistant.",
                input = "What is Portkey"
            )

        asyncio.run(main())
        ```
    </Tab>
</Tabs>

<Note>
  DeepEval's Portkey client traces `chat.completions.create` method.
</Note>
</Step>

<Step title="Run Portkey">

Invoke your agent by executing the script:

```bash
python main.py
```

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Step>

</Steps>

## Advanced Usage

### Logging prompts

If you are [managing prompts](/docs/llm-evaluation/prompt-optimization/prompt-versioning) on Confident AI and wish to log them, pass your `Prompt` to the `create` method.

```python main.py
from portkey_ai import PORTKEY_GATEWAY_URL

from deepeval.openai import OpenAI
from deepeval.prompt import Prompt
from deepeval.tracing import trace

portkey = OpenAI(
  base_url = PORTKEY_GATEWAY_URL,
  api_key = "<PORTKEY_API_KEY>"
)

prompt = Prompt(alias="my_prompt")
prompt.pull(version="00.00.01")

with trace(prompt=prompt):
  response = portkey.chat.completions.create(
      model = "@slug/<model>",
      messages = [
        {"role": "system", "content": prompt.interpolate(name="John")}, # string system prompt
        {"role": "user", "content": "What is Portkey"}
      ],
  )

print(response.choices[0].message.content)
```

### Logging threads

Threads are used to group related traces together, and are useful for chat apps, agents, or any multi-turn interactions. Learn more about [threads](/docs/llm-tracing/advanced-features/threads) here. You can set the `thread_id` in the `trace` context.

```python main.py
from deepeval.openai import OpenAI
from deepeval.tracing import trace

from portkey_ai import PORTKEY_GATEWAY_URL

portkey = OpenAI(
  base_url = PORTKEY_GATEWAY_URL,
  api_key = "<PORTKEY_API_KEY>"
)

with trace(thread_id="test_thread_id_1"):
  response = portkey.chat.completions.create(
      model = "@slug/<model>",
      messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is Portkey"}
      ],
  )

print(response.choices[0].message.content)
```

<Note>
  This is an example of using `STRING` type prompt interpolation.
</Note>


## Evals Usage

### Online evals
If your OpenAI application is in production, and you still want to run evaluations on your traces, use [online evals](/docs/llm-tracing/evaluations). It lets you run evaluations on all incoming traces on Confident AI's server.

<Steps>

<Step title="Create metric collection">

Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your OpenAI agent. Copy the name of the metric collection.

<Frame caption="Create metric collection">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/integrations%3Athird-party-integrations%3Acreate-metric-collection-4k.mp4"
    controls
    autoPlay
  />
</Frame>

</Step>

<Step title="Run evals">

Set the `llm_metric_collection` name in the `trace` context when invoking your OpenAI client to evaluate Llm Spans.

```python main.py
from deepeval.openai import OpenAI
from deepeval.tracing import trace

client = OpenAI()

with trace(llm_metric_collection="test_collection_1"):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello, how are you?"},
        ],
    )
```

</Step>
</Steps>

### End-to-end evals

Confident AI allows you to run [end-to-end evals](/llm-evaluation/single-turn/end-to-end) on your OpenAI client to evaluate your Portkey calls directly. This is recommended if you are testing your Portkey calls in isolation.

<Steps>
<Step title="Create metric">

```python
from deepeval.metrics import AnswerRelevancyMetric

task_completion = AnswerRelevancyMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

<Warning>
  You can only run end-to-end evals on Portkey using metrics that evaluate
  `input`, `output`, or `tools_called`. You can pass parameters like `expected_output`, `expected_tools`, `context` and `retrieval_context` to the `trace` context.
</Warning>

</Step>

<Step title="Run evals">

Replace your `OpenAI` client with DeepEval's. Then, use the dataset's `evals_iterator` to invoke your OpenAI client for each golden. Remember to replace base_url and api_key with the Portkey gateway URL and API key.

<Tabs>
<Tab title="Chat Completions">

```python main.py
from deepeval.openai import OpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset
from deepeval.tracing import trace

client = OpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    with trace(
        llm_metrics=[AnswerRelevancyMetric(), BiasMetric()],
        expected_output=golden.expected_output,
    ):
        client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": golden.input}
            ],
        )
```

</Tab>
<Tab title="Responses">

```python main.py
from deepeval.openai import OpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace

client = OpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    with trace(
        llm_metrics=[AnswerRelevancyMetric(), BiasMetric()],
        expected_output=golden.expected_output,
    ):
        client.responses.create(
            model="gpt-4o",
            instructions="You are a helpful assistant.",
            input=golden.input,
        )
```

</Tab>
<Tab title="Async Chat Completions">

```python main.py
import asyncio
from deepeval.openai import AsyncOpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset
from deepeval.tracing import trace

async_client = AsyncOpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

async def openai_llm_call(input):
    with trace(
        llm_metrics=[AnswerRelevancyMetric(), BiasMetric()],
        expected_output=golden.expected_output,
    ):
        return await async_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful chatbot. Always generate a string response."},
                {"role": "user", "content": input},
            ],
        )

dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    task = asyncio.create_task(openai_llm_call(golden.input))
    dataset.evaluate(task)
```

</Tab>
<Tab title="Async Responses">

```python main.py
import asyncio
from deepeval.openai import AsyncOpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace

async_client = AsyncOpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

async def openai_llm_call(input):
    with trace(
        llm_metrics=[AnswerRelevancyMetric(), BiasMetric()],
        expected_output=golden.expected_output,
    ):
        return await async_client.responses.create(
            model="gpt-4o",
            instructions="You are a helpful assistant.",
            input=input,  
        )


dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    task = asyncio.create_task(openai_llm_call(golden.input))
    dataset.evaluate(task)
```

</Tab>
</Tabs>

<Success>
  This will automatically generate a test run with evaluated Portkey traces using
  inputs from your dataset.
</Success>

</Step>
</Steps>

### Using OpenAI in component-level evals

You can also evaluate Portkey calls through component-level evals. This approach is recommended if you are testing your Portkey calls as a component in a larger application system.

<Steps>

<Step title="Create metric">

```python
from deepeval.metrics import AnswerRelevancyMetric

task_completion = AnswerRelevancyMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

<Warning>
  As with end-to-end evals, you can only use metrics that evaluate `input`,
  `output`, or `tools_called`.
</Warning>

</Step>

<Step title="Run evals">

Replace your `OpenAI` client with DeepEval's. Then, use the dataset's `evals_iterator` to invoke your LLM application for each golden.

<Note>
  Make sure that each function or method in your LLM application is decorated
  with `@observe`.
</Note>

<Tabs>
  <Tab title="Chat Completions">

```python
from deepeval.openai import OpenAI
from deepeval.tracing import observe, trace
from deepeval.dataset import EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric

client = OpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

@observe()
def generate_response(input: str) -> str:
    with trace(
        llm_metrics=[AnswerRelevancyMetric()],
        expected_output=golden.output,
    ):
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": input},
            ],
        )
        return response

# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    generate_response(golden.input)
```

</Tab>
<Tab title="Responses">

```python
from deepeval.openai import OpenAI
from deepeval.tracing import observe, trace
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset

client = OpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

@observe()
def generate_response(input: str) -> str:
    with trace(
        llm_metrics=[AnswerRelevancyMetric()],
        expected_output=golden.expected_output,
    ):
        response = client.responses.create(
            model="gpt-4.1",
            instructions="You are a helpful assistant.",
            input=input,
        )
        return response

# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    generate_response(golden.input)
```

</Tab>
<Tab title="Async Chat Completions">

```python
from deepeval.openai import OpenAI
from deepeval.tracing import observe, trace
from deepeval.dataset import EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric

client = OpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

@observe()
def generate_response(input: str) -> str:
    with trace(
        llm_metrics=[AnswerRelevancyMetric()],
        expected_output=golden.expected_output,
    ):
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": input},
            ],
        )
        return response


# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    generate_response(golden.input)
```

</Tab>
<Tab title="Async Responses">

```python
import asyncio
from deepeval.openai import AsyncOpenAI
from deepeval.tracing import observe, trace
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset

client = AsyncOpenAI(
    base_url = PORTKEY_GATEWAY_URL,
    api_key = "<PORTKEY_API_KEY>"
)

@observe()
async def generate_response(input: str) -> str:
    with trace(
        llm_metrics=[AnswerRelevancyMetric()],
        expected_output=golden.expected_output,
    ):
        response = await client.responses.create(
            model="gpt-4.1",
            instructions="You are a helpful assistant.",
            input=input,
        )
        return response

# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    task = asyncio.create_task(generate_response(golden.input))
    dataset.evaluate(task)
```

</Tab>
</Tabs>

</Step>
</Steps>
---
slug: integrations/third-party/pydantic-ai
subtitle: Use Confident AI for LLM observability and evals for PydanticAI
---

## Overview

[Pydantic AI](https://ai.pydantic.dev/) is a python-native LLM agent framework built on the foundations of pydantic validation. Confident AI allows you to trace and evaluate Pydantic AI agents in just a few lines of code.

## Tracing Quickstart

<Steps>
  <Step title="Install Dependencies">
    Run the following command to install the required packages:
    ```bash
    pip install pydantic-ai sdk -U deepeval
    ```
  </Step>
  <Step title="Configure Pydantic AI">
    Use DeepEval's `ConfidentInstrumentationSettings` to trace the LLM operations. 
    <Tabs>
      <Tab title="Synchronous">
        ```python main.py
        from pydantic_ai import Agent
        from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

        agent = Agent(
            "openai:gpt-4o-mini",
            system_prompt="Be concise, reply with one sentence.",
            instrument=ConfidentInstrumentationSettings(),
            name="test_agent",
        )
            
        agent.run_sync("What are the LLMs?")
        ```
      </Tab>
      <Tab title="Asynchronous">
        ```python main.py
        import asyncio
        from pydantic_ai import Agent
        from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

        agent = Agent(
            "openai:gpt-4o-mini",
            system_prompt="Be concise, reply with one sentence.",
            name="test_agent",
            instrument=ConfidentInstrumentationSettings(),
        )
            
        def execute_simple_agent():
            result = asyncio.run(agent.run("What are the LLMs?"))

        execute_simple_agent()
        ```
      </Tab>
      <Tab title="Streaming">
        ```python main.py
        import asyncio
        from pydantic_ai import Agent
        from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

        agent = Agent(
          "openai:gpt-4o-mini", 
          system_prompt="Be concise, reply with one sentence.", 
          instrument=ConfidentInstrumentationSettings(),
          name="test_agent",
        )

        async def execute_agent_stream():
          async with agent.run_stream("What is the weather in London?") as result:
              async for chunk in result.stream_text(delta=True):
                  print(chunk, end="", flush=True)
              final = await result.get_output()
              print("\n\nFinal:", final)

        asyncio.run(execute_agent_stream())
        ```
      </Tab>
    </Tabs>

  </Step>
  <Step title="Run Pydantic AI">
    Invoke your agent by executing the script:
    ```bash
    python main.py
    ```
    You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.
  </Step>
</Steps>

## Advanced Usage

### Logging prompts

If you are [managing prompts](/docs/llm-evaluation/prompt-optimization/prompt-versioning) on Confident AI and wish to log them, pass your `Prompt` object to the ConfidentInstrumentationSettings.

```python main.py
from pydantic_ai import Agent
from deepeval.prompt import Prompt
from deepeval.integrations.pydantic_ai.instrumentator import ConfidentInstrumentationSettings

prompt = Prompt(alias="my-prompt")
prompt.pull(version="00.00.01")

system_prompt = prompt.interpolate()

agent = Agent(
    "openai:gpt-4o-mini",
    system_prompt=system_prompt,
    instrument=ConfidentInstrumentationSettings(
        confident_prompt=prompt,
    )
)
    
result = agent.run_sync("What are the LLMs?")
```

<Note>
  Logging prompts lets you attribute specific prompts to OpenAI Agent LLM spans.
  Be sure to **pull the prompt** before logging it, otherwise the prompt will
  not be visible on Confident AI.
</Note>

### Logging threads

Threads are used to group related traces together, and are useful for chat apps, agents, or any multi-turn interactions. You can learn more about [threads](/docs/llm-tracing/advanced-features/threads) here. Pass the `thread_id` to the `ConfidentInstrumentationSettings`.

<CodeBlocks>
  <CodeBlock title="Synchronous">
    ```python
    from pydantic_ai import Agent
    from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

    agent = Agent(
        model="openai:gpt-4o-mini",
        system_prompt="Be concise, reply with one sentence.",
        instrument=ConfidentInstrumentationSettings(
            thread_id="test_thread_id_1",
        )
    )

    result = agent.run_sync("What are the LLMs?")
    ```

  </CodeBlock>
  <CodeBlock title="Asynchronous">
    ```python
    from pydantic_ai import Agent
    from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings
    
    agent = Agent(
        model="openai:gpt-4o-mini",
        system_prompt="Be concise, reply with one sentence.",
        instrument=ConfidentInstrumentationSettings(
            thread_id="test_thread_id_1",
        )
    )

    async def execute_agent():
        return await agent.run("What are the LLMs?")

    result = asyncio.run(execute_agent())
    ```

  </CodeBlock>
  <CodeBlock title="Streaming">
    ```python
    from pydantic_ai import Agent
    from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

    agent = Agent(
        model="openai:gpt-4o-mini",
        system_prompt="Be concise, reply with one sentence.",
        instrument=ConfidentInstrumentationSettings(
            thread_id="test_thread_id_1",
        )
    )

    async def execute_agent_stream():
      async with agent.run_stream("What are the LLMs?") as result:
          async for chunk in result.stream_text(delta=True):
              print(chunk, end="", flush=True)
          final = await result.get_output()
          print("\n\nFinal:", final)

    asyncio.run(execute_agent_stream())
    ```

  </CodeBlock>
</CodeBlocks>

### Trace attributes

Other trace attributes can also be passed to the `ConfidentInstrumentationSettings`.

<CodeBlocks>
  <CodeBlock title="Synchronous">
    ```python
    from pydantic_ai import Agent
    from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings
    
    agent = Agent(
        model="openai:gpt-4o-mini",
        system_prompt="Be concise, reply with one sentence.",
        instrument=ConfidentInstrumentationSettings(
            thread_id="test_thread_id_1",
            name="Name of Trace",
            tags=["Tag 1", "Tag 2"],
            metadata={"Key": "Value"},
            user_id="user_1",
        )
    )
    ```

  </CodeBlock>
  <CodeBlock title="Asynchronous">
    ```python
    import asyncio
    from pydantic_ai import Agent
    from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

    agent = Agent(
        model="openai:gpt-4o-mini",
        system_prompt="Be concise, reply with one sentence.",
        instrument=ConfidentInstrumentationSettings(
            thread_id="test_thread_id_1",
            name="Name of Trace",
            tags=["Tag 1", "Tag 2"],
            metadata={"Key": "Value"},
            user_id="user_1",
        )
    )

    async def execute_agent():
        return await agent.run(user_prompt="What are the LLMs?")

    result = asyncio.run(execute_agent())
    ```

  </CodeBlock>
  <CodeBlock title="Streaming">
    ```python
    import asyncio
    from pydantic_ai import Agent
    from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

    agent = Agent(
        model="openai:gpt-4o-mini",
        system_prompt="Be concise, reply with one sentence.",
        instrument=ConfidentInstrumentationSettings(
            name="Name of Trace",
            tags=["Tag 1", "Tag 2"],
            metadata={"Key": "Value"},
            user_id="user_1",
            thread_id="test_thread_id_1",
        )
    )

    async def execute_agent_stream():
      async with agent.run_stream(
        user_prompt="What are the LLMs?",
      ) as result:

          async for chunk in result.stream_text(delta=True):
              print(chunk, end="", flush=True)
          final = await result.get_output()
          print("\n\nFinal:", final)

    asyncio.run(execute_agent_stream())
    ```

  </CodeBlock>
</CodeBlocks>

<Accordion title='View Trace Attributes'>
<ParamField path="name" type="str" required={false}>
  The name of the trace. [Learn more](/docs/llm-tracing/advanced-features/name).
</ParamField>

<ParamField path="tags" type="List[str]" required={false}>
  Tags are string labels that help you group related traces. [Learn
  more](/docs/llm-tracing/advanced-features/tags).
</ParamField>

<ParamField path="metadata" type="Dict" required={false}>
  Attach any metadata to the trace. [Learn
  more](/docs/llm-tracing/advanced-features/metadata).
</ParamField>

<ParamField path="thread_id" type="str" required={false}>
  Supply the thread or conversation ID to view and evaluate conversations.
  [Learn more](/docs/llm-tracing/advanced-features/threads).
</ParamField>

<ParamField path="user_id" type="str" required={false}>
  Supply the user ID to enable user analytics. [Learn
  more](/docs/llm-tracing/advanced-features/users).
</ParamField>

<Info>
  Each attribute is **optional**, and works the same way as the [native tracing
  features](/docs/llm-tracing/introduction) on Confident AI.
</Info>
</Accordion>

### Sending annotations

Send human annotations on the threads or traces on Confident AI. Learn more about [sending annotations](/docs/human-in-the-loop/collect-feedback).

<Tabs>
  <Tab title="Traces">
    ```python
    from deepeval.tracing import trace
    from deepeval.annotation import send_annotation
    ...

    TRACE_UUID = None
    with trace() as current_trace:
        result = agent.run_sync("What are the LLMs?")
        TRACE_UUID = current_trace.uuid # you can save this to use it later
    
    send_annotation(
        trace_uuid=TRACE_UUID,
        rating=1,
    )

    ```
  </Tab>
  <Tab title="Threads">
    ```python
    from deepeval.tracing import trace
    from deepeval.annotation import send_annotation
    ...

    THREAD_ID = "test_thread_id_1"
    with trace(thread_id=THREAD_ID) as current_trace:
        result = agent.run_sync("What are the LLMs?")

    send_annotation(
        thread_id=THREAD_ID,
        rating=1,
    )
    ```
  </Tab>
</Tabs>

## Evals Usage

### Online evals

You can run [online evals](/docs/llm-tracing/evaluations) on your Pydantic agent, which will run evaluations on all incoming traces on Confident AI’s servers. This approach is recommended if your agent is in production.

<Steps>
  <Step title="Create metric collection">
    Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your Pydantic agent.
    <Frame caption="Create metric collection">
      <video
        autoPlay
        loop
        muted
        src="https://confident-docs.s3.us-east-1.amazonaws.com/metrics:create-collection-4k.mp4"
        type="video/mp4"
      />
    </Frame>
    <Warning>
      Your metric collection must only contain metrics that only evaluate the input and actual output of your Pydantic AI agent.
    </Warning>
  </Step>
  <Step title="Run evals">
    You can run evals at both the trace and span level. We recommend creating separate [metric collections](/docs/metrics/metric-collections) for each component, since each requires its own evaluation criteria and metrics.
    After instrumenting your Pydantic AI pass the metric collection name to the respective componens:
    <Tabs>

      <Tab title="Trace">
        Pass the `metric_collection` parameter to the `ConfidentInstrumentationSettings`.

        ```python main.py
        from pydantic_ai import Agent
        from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

        agent = Agent(
            model="openai:gpt-4o-mini",
            system_prompt="Be concise, reply with one sentence.",
            instrument=ConfidentInstrumentationSettings(
                trace_metric_collection="test_collection_1",
            )
        )
            
        result = agent.run_sync("What are the LLMs?")
        ```
      </Tab>

      <Tab title="Agent Span">
        Pass the `agent_metric_collection` parameter to the `ConfidentInstrumentationSettings`.

        ```python main.py
        from pydantic_ai import Agent
        from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

        agent = Agent(
            model="openai:gpt-4o-mini",
            system_prompt="Be concise, reply with one sentence.",
            instrument=ConfidentInstrumentationSettings(
                agent_metric_collection="test_collection_1",
            )
        )
            
        result = agent.run_sync("What are the LLMs?")
        ```
      </Tab>

      <Tab title="LLM Span">
        Pass the `llm_metric_collection` parameter to the `ConfidentInstrumentationSettings`.
        ```python main.py
        from pydantic_ai import Agent
        from deepeval.integrations.pydantic_ai import ConfidentInstrumentationSettings

        agent = Agent(
            model="openai:gpt-4o-mini",
            system_prompt="Be concise, reply with one sentence.",
            instrument=ConfidentInstrumentationSettings(
                llm_metric_collection="test_collection_1",
            )
        )
            
        result = agent.run_sync("What are the LLMs?")
        ```
      </Tab>

      <Tab title="Tool Span">
        Pass the `tool_metric_collection_map` parameter to the `ConfidentInstrumentationSettings`. This maps the tool name to the metric collection name.

        ```python main.py
        from pydantic_ai import Agent
        from deepeval.integrations.pydantic_ai.instrumentator import ConfidentInstrumentationSettings

        agent = Agent(
            model="openai:gpt-4o-mini",
            system_prompt="Be concise, reply with one sentence.",
            instrument=ConfidentInstrumentationSettings(
                tool_metric_collection_map={"get_llm_info": "test_collection_1"},
            )
        )

        @agent.tool
        def get_llm_info() -> str:
            return "LLMs are the best!"
            
        result = agent.run_sync("What are the LLMs?")
        ```
      </Tab>
    </Tabs>

  </Step>
  <Success>
    All incoming traces will now be evaluated using metrics from your metric
    collection.
  </Success>
</Steps>

### End-to-end evals

Running [end-to-end evals](/docs/llm-evaluation/single-turn/end-to-end) on your Pydantic agent evaluates your agent locally, and is the recommended approach if your agent is in a development or testing environment.

<Steps>
  <Step title="Create metric">
    ```python
    from deepeval.metrics import AnswerRelevancyMetric

    answer_relevancy = AnswerRelevancyMetric(
        threshold=0.7,
        model="gpt-4o-mini",
        include_reason=True
    )
    ```

  </Step>
  <Warning>
    Similar to online evals, you can only run end-to-end evals on metrics that
    evaluate the input and actual output of your Pydantic agent.
  </Warning>
  <Step title="Run evals">
    As shown in online evals, you can provide `metrics` to different components of the Agent, similar to the `metric_collection`. Then, use the dataset's `evals_iterator` to invoke your Pydantic agent for each golden.
    <Tabs>
      <Tab title="Asynchronous">
        ```python main.py maxLines=100
        import asyncio
        from deepeval.integrations.pydantic_ai import Agent
        from deepeval.metrics import AnswerRelevancyMetric
        from deepeval.dataset import EvaluationDataset, Golden

        agent = Agent("openai:gpt-4o-mini", system_prompt="Be concise, reply with one sentence.")

        answer_relavancy_metric = AnswerRelevancyMetric()
        dataset = EvaluationDataset(goldens=[Golden(input="What's 7 * 8?"), Golden(input="What's 7 * 6?")])

        for golden in dataset.evals_iterator():
            task = asyncio.create_task(agent.run(golden.input, metrics=[answer_relavancy_metric]))
            dataset.evaluate(task)
        ```
      </Tab>
    </Tabs>
    <Success>
      This will automatically generate a test run with evaluated traces using inputs
      from your dataset.
    </Success>

  </Step>
</Steps>

You can view evals on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

<Frame>
  <video
    src="https://confident-bucket.s3.us-east-1.amazonaws.com/end-to-end%3Apydantic-1080.mp4"
    controls
    autoPlay
    playsInline
  />
</Frame>
````
